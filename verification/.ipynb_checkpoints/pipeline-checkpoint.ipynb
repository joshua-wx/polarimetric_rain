{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.ndimage import map_coordinates\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Need to be more careful when processing RF3 and DP data - need to reject RF3 pixel where there is a no DP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = 15\n",
    "radar_id = 31 #2,64,66,71,31,32,95\n",
    "date_start_str = '20200401'\n",
    "date_end_str = '20200531'\n",
    "dp_path = '/g/data/rq0/admin/dprain-verify-latest/rfgrid'\n",
    "rf3_path = '/g/data/rq0/admin/dprain-verify-orignal/rf3_rrate'\n",
    "of_path = '/g/data/rq0/admin/dprain-verify-orignal/optflow'\n",
    "gauge_path = '/g/data/rq0/admin/dprain-verify-orignal/gaugeobs'\n",
    "\n",
    "out_path = '/g/data/kl02/jss548/PST/dprain-verify'\n",
    "range_limit = 100 #km\n",
    "min_precip = 2.5 #mm in 15min\n",
    "grid_res = 500 #m\n",
    "\n",
    "radar_id_str = f'{radar_id:02}'\n",
    "\n",
    "rf3_gap_fill = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from l.\n",
    "    From http://stackoverflow.com/a/312464\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    \"\"\"\n",
    "    Generate date list between dates\n",
    "    \"\"\"\n",
    "    date_list = []\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        date_list.append(date1 + timedelta(n))\n",
    "    return date_list\n",
    "\n",
    "def advection(rrate1, rrate2,\n",
    "              oflow1_pix, oflow2_pix,\n",
    "              T_start=0, T_end=6,\n",
    "              T=6, t=1):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "    Returns the accumulated rainfall amount by applying a time weighted interpolation from radar volume 1 to radar volume 2\n",
    "    using optical flow fields (u,v and both timesteps) and an interpolation timestep of t. Can return r_acc only for a portion of the timespan\n",
    "    between radar volumes 1 and 2.\n",
    "    \n",
    "    HELP\n",
    "    note: first radar volume is the oldest. second radar volume is the newest.\n",
    "\n",
    "    INPUTS\n",
    "    rrate1/rrate2: rain rate (mm/hr) for first radar volume and second radar volume\n",
    "    oflow1_pix/oflow2_pix: optical flow (pix/min) in [u and v] direction for first radar volume and second radar volume (list of length 2, elements are 2D np.array)\n",
    "    T_start: starting timestep (minutes from radar volume 1) for interpolation (0 will include the first radar timestep)\n",
    "    T_end: ending timestep (minutes from radar volume 2) for interpolation (T_end=T will include the second radar timestep)\n",
    "    T: time difference between radar volumes (minutes)\n",
    "    t: timestep for interpolation (minutes)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    r_acc: accumulated rainfall totals (mm)\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform temporal interpolation\n",
    "    r_acc = np.zeros((rrate1.shape))\n",
    "    x, y = np.meshgrid(\n",
    "        np.arange(rrate1.shape[1], dtype=float), np.arange(rrate1.shape[0], dtype=float)\n",
    "    )\n",
    "    for i in range(T_start, T_end + t, t):\n",
    "        #shift timestep 1 forwards (this is the older timestep)\n",
    "        ts_forward = -i\n",
    "        y1_shift = y + (ts_forward * oflow1_pix[1])\n",
    "        x1_shift = x + (ts_forward * oflow1_pix[0])\n",
    "        pos1 = (y1_shift, x1_shift)\n",
    "        R1 = map_coordinates(rrate1, pos1, order=1)\n",
    "        weight1 = (T - i)/T\n",
    "\n",
    "        #shift timestep 2 backwards (this is the newer timestep)\n",
    "        ts_backwards = (T-i)\n",
    "        y2_shift = y + (ts_backwards * oflow2_pix[1])\n",
    "        x2_shift = x + (ts_backwards * oflow2_pix[0])\n",
    "        pos2 = (y2_shift, x2_shift)\n",
    "        R2 = map_coordinates(rrate2, pos2, order=1)\n",
    "        weight2 = i/T\n",
    "\n",
    "        #weighted combination\n",
    "        rrate_interp = R1 * weight1 + R2 * weight2\n",
    "\n",
    "        #convert to mm/hr in mm\n",
    "        r_acc += rrate_interp/60*t\n",
    "        \n",
    "    return r_acc\n",
    "\n",
    "\n",
    "def calc_distance(radar_lat, radar_lon, site_lat_list, site_lon_list):\n",
    "    #Haversine formula\n",
    "    #radius of earth\n",
    "    R = 6373.0\n",
    "    #convert to radians\n",
    "    radar_lat = np.radians(radar_lat)\n",
    "    radar_lon = np.radians(radar_lon)\n",
    "    site_lat_list = np.radians(site_lat_list)\n",
    "    site_lon_list = np.radians(site_lon_list)\n",
    "    #formulation\n",
    "    dlon = site_lon_list - radar_lon\n",
    "    dlat = site_lat_list - radar_lat\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(radar_lat) * np.cos(site_lat_list) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def datetime_from_list(ffn_list):\n",
    "    \n",
    "    dt_list = []\n",
    "    for item in ffn_list:\n",
    "        basename = os.path.basename(item)\n",
    "        parts = basename.split('_')\n",
    "        date_str = parts[1]\n",
    "        time_str = parts[2][0:6]\n",
    "        dt_list.append(datetime.strptime(date_str + '_' + time_str, '%Y%m%d_%H%M%S'))\n",
    "    return dt_list\n",
    "\n",
    "def read_file(dt, file_type):\n",
    "    if file_type=='rf3':\n",
    "        nc_ffn = f'{rf3_path}/{radar_id_str}/{dt.year}/{dt.month:02}/{dt.day:02}/{radar_id}_{dt.strftime(\"%Y%m%d_%H%M%S\")}.prcp-rrate.nc'\n",
    "    elif file_type=='dp':\n",
    "        #ignore sectonds on dp file\n",
    "        try:\n",
    "            nc_ffn = glob(f'{dp_path}/{radar_id_str}/{dt.year}{dt.month:02}{dt.day:02}/{radar_id}_{dt.strftime(\"%Y%m%d_%H%M\")}*.prcp-rrate.nc')[0]\n",
    "        except:\n",
    "            print('no files found on:', f'{dp_path}/{radar_id_str}/{dt.year}{dt.month:02}{dt.day:02}/{radar_id}_{dt.strftime(\"%Y%m%d_%H%M\")}*.prcp-rrate.nc')\n",
    "            return None\n",
    "    elif file_type=='of':\n",
    "        nc_ffn = f'{of_path}/{radar_id_str}/{dt.year}/{dt.month:02}/{dt.day:02}/{radar_id}_{dt.strftime(\"%Y%m%d_%H%M%S\")}.optflow.nc'\n",
    "    try:\n",
    "        with xr.open_dataset(nc_ffn) as ds:\n",
    "            if file_type=='rf3' or file_type=='dp': \n",
    "                rrate = ds['rain_rate'].data\n",
    "                #rrate[rrate==0] = np.nan\n",
    "                return rrate\n",
    "            elif file_type=='of':\n",
    "                return [ds['flow_u'].data*60/grid_res, ds['flow_v'].data*60/grid_res]\n",
    "    except:\n",
    "        print('missing file:', nc_ffn)\n",
    "        return None \n",
    "\n",
    "def run_advection(vol_dt_list, vol_idx, file_type, gauge_start_dt64, gauge_end_dt64):    \n",
    "\n",
    "    #open inital dataset\n",
    "    dt1 = vol_dt_list[vol_idx[0]]\n",
    "    rrate1 = read_file(dt1, file_type)\n",
    "    oflow1_pix = read_file(dt1, 'of')\n",
    "    #abort on errors\n",
    "    if rrate1 is None or oflow1_pix is None:\n",
    "        return None, True\n",
    "    \n",
    "    acrain = np.zeros_like(rrate1)\n",
    "    #advect rainrate files\n",
    "    for idx in vol_idx[1:]:\n",
    "        dt2 = vol_dt_list[idx]\n",
    "        \n",
    "        #open next dataset\n",
    "        rrate2 = read_file(dt2, file_type)\n",
    "        oflow2_pix = read_file(dt2, 'of')         \n",
    "        #abort on errors\n",
    "        if rrate2 is None or oflow2_pix is None:\n",
    "            return None, True  \n",
    "        \n",
    "        #time\n",
    "        dt64_1 = np.array(dt1, dtype='datetime64[m]')\n",
    "        dt64_2 = np.array(dt2, dtype='datetime64[m]')\n",
    "        td_m = (dt64_2-dt64_1)/np.timedelta64(1, 'm')\n",
    "        \n",
    "        #check for volumes outside the gauge times\n",
    "        start_diff_min = (dt64_1-gauge_start_dt64)/np.timedelta64(1, 'm')\n",
    "        end_diff_min = (gauge_end_dt64-dt64_2)/np.timedelta64(1, 'm')\n",
    "        T_start = 0\n",
    "        T_end = int(td_m)\n",
    "        skip = False\n",
    "        if start_diff_min <= -td_m:\n",
    "            skip = True\n",
    "        elif start_diff_min < 0:\n",
    "            T_start = int(start_diff_min)\n",
    "        if end_diff_min <= -td_m:\n",
    "            skip=True\n",
    "        elif end_diff_min < 0:\n",
    "            T_end = int(end_diff_min)\n",
    "            \n",
    "        #run accumulations\n",
    "        if not skip:\n",
    "            acrain += advection(rrate1, rrate2,\n",
    "                             oflow1_pix, oflow2_pix,\n",
    "                             T_start=T_start, T_end=T_end,\n",
    "                             T=td_m, t=1)\n",
    "        \n",
    "        #move next to prev\n",
    "        dt1 = dt2\n",
    "        rrate1 = rrate2\n",
    "        oflow1_pix = oflow2_pix\n",
    "        \n",
    "    return acrain, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(date_dt):\n",
    "    \n",
    "    #init\n",
    "    date_str = date_dt.strftime('%Y%m%d')\n",
    "    #build paths\n",
    "    gauge_folder = f'{gauge_path}/{date_dt.year}/{date_dt.month:02}/{date_dt.day:02}'\n",
    "    of_folder = f'{of_path}/{radar_id_str}/{date_dt.year}/{date_dt.month:02}/{date_dt.day:02}'\n",
    "    rr_orig_folder = f'{rf3_path}/{radar_id_str}/{date_dt.year}/{date_dt.month:02}/{date_dt.day:02}'\n",
    "    rr_dp_folder = f'{dp_path}/{radar_id_str}/{date_dt.year}{date_dt.month:02}{date_dt.day:02}'\n",
    "    #list files\n",
    "    gauge_fflist = sorted(glob(gauge_folder + '/*.nc'))\n",
    "    of_fflist = sorted(glob(of_folder + '/*.nc'))\n",
    "    rr_orig_fflist = sorted(glob(rr_orig_folder + '/*.nc'))\n",
    "    rr_dp_fflist = sorted(glob(rr_dp_folder + '/*.nc'))\n",
    "    \n",
    "    #read radar latlon\n",
    "    with xr.open_dataset(rr_orig_fflist[0]) as ds:\n",
    "        radar_lat = ds.proj.latitude_of_projection_origin\n",
    "        radar_lon = ds.proj.longitude_of_central_meridian\n",
    "        x_grid, y_grid = np.meshgrid(ds.x.data, ds.y.data)\n",
    "    #create latlon grid\n",
    "    proj = pyproj.Proj(proj='aea', lat_1=ds.proj.standard_parallel[0], lat_2=ds.proj.standard_parallel[1],\n",
    "                       lat_0=ds.proj.latitude_of_projection_origin, lon_0=ds.proj.longitude_of_central_meridian,\n",
    "                       x_0=0, y_0=0)\n",
    "    lon_grid, lat_grid = proj(x_grid*1000, y_grid*1000, inverse=True)\n",
    "\n",
    "    #init\n",
    "    gauge_lat_out = []\n",
    "    gauge_lon_out = []\n",
    "    gauge_rain_out = []\n",
    "    gauge_dt64_out = []\n",
    "    rf3_acrain_out = []\n",
    "    dp_acrain_out = []\n",
    "\n",
    "    rf_replace = 0\n",
    "    n_gaugeobs = len(gauge_fflist)\n",
    "    for i in range(n_gaugeobs):\n",
    "\n",
    "        ##############################\n",
    "        # Gauge filtering\n",
    "        ##############################\n",
    "        #load gauge\n",
    "        gauge_ds = xr.open_dataset(gauge_fflist[i])\n",
    "        #find stations nearby\n",
    "        gauge_id = gauge_ds['station_id'].data\n",
    "        gauge_lat = gauge_ds['latitude'].data\n",
    "        gauge_lon = gauge_ds['longitude'].data\n",
    "        gauge_dist = calc_distance(radar_lat, radar_lon, gauge_lat, gauge_lon)\n",
    "        gauge_dist_mask = gauge_dist <= range_limit\n",
    "        #apply gauge lower thresholds ########## to change later\n",
    "        gauge_rain = gauge_ds['precipitation'].data\n",
    "        gauge_rain_mask = gauge_rain >= min_precip\n",
    "        #final index\n",
    "        gauge_idx = np.where(np.logical_and(gauge_dist_mask, gauge_rain_mask))[0]\n",
    "        #skip timestep if there's no gauges to process\n",
    "        if len(gauge_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        ##############################\n",
    "        # find volumes to process for gauge timestep\n",
    "        ##############################\n",
    "        #find volume dates in range (and file before for rain advection)\n",
    "        #read time of gauge dataset\n",
    "        start_dt64 = np.array(gauge_ds['start_time'].data, dtype='datetime64[m]')\n",
    "        valid_dt64 = np.array(gauge_ds['valid_time'].data, dtype='datetime64[m]')\n",
    "        start_dt_parts = np.datetime_as_string(start_dt64, unit='D').split('-')\n",
    "        valid_dt_parts = np.datetime_as_string(valid_dt64, unit='D').split('-')\n",
    "        #list volumes\n",
    "        start_folder = f'{rf3_path}/{radar_id_str}/{start_dt_parts[0]}/{start_dt_parts[1]}/{start_dt_parts[2]}'\n",
    "        end_folder = f'{rf3_path}/{radar_id_str}/{valid_dt_parts[0]}/{valid_dt_parts[1]}/{valid_dt_parts[2]}'\n",
    "        fflist = sorted(set(glob(start_folder + '/*.nc') + glob(end_folder + '/*.nc')))\n",
    "        #convert filenames to datetimes\n",
    "        vol_dt_list = datetime_from_list(fflist)\n",
    "        vol_dt64_array = np.array(vol_dt_list, dtype='datetime64')\n",
    "        #filter\n",
    "        vol_idx = np.where(np.logical_and(vol_dt64_array>=start_dt64, vol_dt64_array<=valid_dt64))[0]\n",
    "        if len(vol_idx) < 2:\n",
    "            continue\n",
    "            \n",
    "        #append trailing index for advection\n",
    "        vol_idx = np.append([vol_idx[0]-1], vol_idx)\n",
    "        vol_idx = np.append(vol_idx, [vol_idx[-1]+1])\n",
    "\n",
    "        ##############################\n",
    "        # generate radar accumulations\n",
    "        ##############################\n",
    "        rf3_acrain, error_flag = run_advection(vol_dt_list, vol_idx, 'rf3', start_dt64, valid_dt64)\n",
    "        dp_acrain, error_flag  = run_advection(vol_dt_list, vol_idx, 'dp', start_dt64, valid_dt64)\n",
    "        \n",
    "        if error_flag==True or error_flag==True:\n",
    "            #skip matching due to missing data\n",
    "            continue\n",
    "        \n",
    "        #match gauge site to radar site\n",
    "        for idx in gauge_idx:\n",
    "            #exact gauge information\n",
    "            g_id = gauge_id[idx]\n",
    "            g_lat = gauge_lat[idx]\n",
    "            g_lon = gauge_lon[idx]\n",
    "            g_rain = gauge_rain[idx]\n",
    "            #cost function for nearest radar grid point\n",
    "            cost = np.sqrt((lon_grid - g_lon)**2 \\\n",
    "                    + (lat_grid - g_lat)**2) #A cost function for searching\n",
    "            grid_idx = np.where(cost == cost.min())\n",
    "\n",
    "            rf3_value = rf3_acrain[grid_idx][0]\n",
    "            dp_value = dp_acrain[grid_idx][0]\n",
    "            \n",
    "            #use RF3 for gaps\n",
    "            if np.isnan(dp_value) and rf3_gap_fill:\n",
    "                dp_value = rf3_value\n",
    "                rf_replace += 1\n",
    "                \n",
    "            #append if not nan\n",
    "            if not np.isnan(rf3_value) and not np.isnan(dp_value) and rf3_value>0 and dp_value>0:\n",
    "                gauge_lat_out.append(g_lat)\n",
    "                gauge_lon_out.append(g_lon)\n",
    "                gauge_rain_out.append(g_rain)\n",
    "                rf3_acrain_out.append(rf3_value)\n",
    "                dp_acrain_out.append(dp_value)\n",
    "                gauge_dt64_out.append(start_dt64)\n",
    "            else:\n",
    "                print('no valid data')\n",
    "            \n",
    "    #save to file\n",
    "    if len(gauge_rain_out) > 0:\n",
    "        out_folder = f'{out_path}/{radar_id_str}/'\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.makedirs(out_folder)\n",
    "        out_ffn = f'{out_folder}/{radar_id_str}_{date_str}.npz'\n",
    "        np.savez(out_ffn, gauge_rain=gauge_rain_out, rf3_acrain=rf3_acrain_out, \n",
    "                 dp_acrain=dp_acrain_out, gauge_dt64=gauge_dt64_out, \n",
    "                 gauge_lat=gauge_lat_out, gauge_lon=gauge_lon_out,\n",
    "                 rf_replace=rf_replace)\n",
    "    print('processed', date_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2020-04-12 00:00:00\n",
      "processed 2020-04-08 00:00:00\n",
      "processed 2020-04-06 00:00:00\n",
      "processed 2020-04-14 00:00:00\n",
      "processed 2020-04-03 00:00:00\n",
      "processed 2020-04-07 00:00:00\n",
      "processed 2020-04-15 00:00:00\n",
      "processed 2020-04-02 00:00:00\n",
      "processed 2020-04-01 00:00:00\n",
      "processed 2020-04-11 00:00:00\n",
      "processed 2020-04-05 00:00:00\n",
      "processed 2020-04-04 00:00:00\n",
      "processed 2020-04-10 00:00:00\n",
      "processed 2020-04-13 00:00:00\n",
      "processed 2020-04-09 00:00:00\n",
      "processed: 24.59\n",
      "processed 2020-04-18 00:00:00\n",
      "processed 2020-04-20 00:00:00\n",
      "processed 2020-04-27 00:00:00\n",
      "processed 2020-04-23 00:00:00\n",
      "processed 2020-04-29 00:00:00\n",
      "processed 2020-04-30 00:00:00\n",
      "processed 2020-04-26 00:00:00\n",
      "processed 2020-04-17 00:00:00\n",
      "processed 2020-04-16 00:00:00\n",
      "processed 2020-04-22 00:00:00\n",
      "processed 2020-04-25 00:00:00\n",
      "processed 2020-04-21 00:00:00processed\n",
      " 2020-04-28 00:00:00\n",
      "processed 2020-04-24 00:00:00\n",
      "processed 2020-04-19 00:00:00\n",
      "processed: 49.18\n"
     ]
    }
   ],
   "source": [
    "#manager\n",
    "dt_start = datetime.strptime(date_start_str, '%Y%m%d')\n",
    "dt_end = datetime.strptime(date_end_str, '%Y%m%d')\n",
    "dt_list = daterange(dt_start, dt_end)\n",
    "\n",
    "# # run single processing\n",
    "# for dt in dt_list:\n",
    "#     worker(dt)\n",
    "\n",
    "#run multiproc\n",
    "i            = 0\n",
    "n_dates      = len(dt_list) \n",
    "\n",
    "for dtlist_chunk in chunks(dt_list, NCPU): #CUSTOM RANGE USED\n",
    "    with Pool(NCPU) as pool:\n",
    "        _ = pool.map(worker, dtlist_chunk)\n",
    "    i += NCPU\n",
    "    print('processed: ' + str(round(i/n_dates*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:radar-dev] *",
   "language": "python",
   "name": "conda-env-radar-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
